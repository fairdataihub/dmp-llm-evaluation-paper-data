{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b53498c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved to participant_dmp_assignments.csv with 10 records\n",
      "âœ… All rows have distinct numbers across categories.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import csv\n",
    "\n",
    "# ---- Config ----\n",
    "categories = [\"X\", \"Y\", \"Z\"]\n",
    "num_dmps_per_category = 10\n",
    "num_participants = 10  # total desired\n",
    "batch_size = num_dmps_per_category  # 10 participants per batch\n",
    "random.seed()  # set random.seed(42) for reproducibility\n",
    "\n",
    "assignments = []\n",
    "participant_id = 1\n",
    "\n",
    "# Generate exactly num_participants rows\n",
    "while participant_id <= num_participants:\n",
    "    # Create one shuffled base permutation for this batch\n",
    "    base = list(range(1, num_dmps_per_category + 1))\n",
    "    random.shuffle(base)\n",
    "\n",
    "    # Shifts ensure different numbers per row\n",
    "    shifts = {\"X\": 0, \"Y\": 1, \"Z\": 2}\n",
    "\n",
    "    # Fill this batch (or until we reach total participants)\n",
    "    for i in range(batch_size):\n",
    "        if participant_id > num_participants:\n",
    "            break  # stop if we already reached the total\n",
    "        row = {}\n",
    "        for cat in categories:\n",
    "            num = base[(i + shifts[cat]) % num_dmps_per_category]\n",
    "            row[cat] = f\"{cat}_{num}.md\"\n",
    "\n",
    "        assignments.append({\n",
    "            \"Participant number\": participant_id,\n",
    "            \"DMP1\": row[\"X\"],\n",
    "            \"DMP2\": row[\"Y\"],\n",
    "            \"DMP3\": row[\"Z\"],\n",
    "        })\n",
    "        participant_id += 1\n",
    "\n",
    "# ---- Save to CSV ----\n",
    "with open(\"participant_dmp_assignments.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"Participant number\", \"DMP1\", \"DMP2\", \"DMP3\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(assignments)\n",
    "\n",
    "print(\"âœ… Saved to participant_dmp_assignments.csv with\", len(assignments), \"records\")\n",
    "\n",
    "# ---- Validation ----\n",
    "def check_row_number_uniqueness(assignments):\n",
    "    ok = True\n",
    "    for row in assignments:\n",
    "        nums = []\n",
    "        for k in (\"DMP1\",\"DMP2\",\"DMP3\"):\n",
    "            _, num = row[k].split(\"_\")\n",
    "            nums.append(int(num.replace(\".md\",\"\")))\n",
    "        if len(set(nums)) != 3:\n",
    "            ok = False\n",
    "            print(f\"âŒ Row {row['Participant number']}: duplicate number(s) across categories -> {nums}\")\n",
    "    if ok:\n",
    "        print(\"âœ… All rows have distinct numbers across categories.\")\n",
    "    return ok\n",
    "\n",
    "check_row_number_uniqueness(assignments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d72f326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved participant_dmp_assignments.csv (30 records)\n",
      "âœ… Updated dmp_links.csv: 30 total rows (0 new).\n",
      "   ðŸ‘‰ Open this CSV and paste your Google Drive PDF links in the 'pdf_link' column.\n",
      "âœ… Saved participant_dmp_assignments_with_links.csv (joined with links)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import csv\n",
    "from math import ceil\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "CATEGORIES = [\"X\", \"Y\", \"Z\"]\n",
    "NUM_DMPS_PER_CATEGORY = 10\n",
    "NUM_PARTICIPANTS = 30\n",
    "BATCH_SIZE = NUM_DMPS_PER_CATEGORY  # 10 participants per batch\n",
    "LINKS_CSV = \"dmp_links.csv\"         # master link table you edit by hand\n",
    "ASSIGN_CSV = \"participant_dmp_assignments.csv\"\n",
    "ASSIGN_WITH_LINKS_CSV = \"participant_dmp_assignments_with_links.csv\"\n",
    "\n",
    "random.seed()  # set e.g. random.seed(42) for reproducibility\n",
    "\n",
    "# ---------------- Assignment generation (rotation ensures distinct numbers per row) ----------------\n",
    "assignments = []\n",
    "participant_id = 1\n",
    "num_batches = ceil(NUM_PARTICIPANTS / BATCH_SIZE)\n",
    "\n",
    "for b in range(num_batches):\n",
    "    remaining = NUM_PARTICIPANTS - participant_id + 1\n",
    "    this_batch_size = min(BATCH_SIZE, remaining)\n",
    "\n",
    "    base = list(range(1, NUM_DMPS_PER_CATEGORY + 1))\n",
    "    random.shuffle(base)\n",
    "\n",
    "    shifts = {\"X\": 0, \"Y\": 1, \"Z\": 2}  # fixed rotation\n",
    "\n",
    "    for i in range(this_batch_size):\n",
    "        row = {}\n",
    "        for cat in CATEGORIES:\n",
    "            num = base[(i + shifts[cat]) % NUM_DMPS_PER_CATEGORY]\n",
    "            row[cat] = f\"{cat}_{num}.md\"\n",
    "\n",
    "        assignments.append({\n",
    "            \"Participant number\": participant_id,\n",
    "            \"DMP1\": row[\"X\"],\n",
    "            \"DMP2\": row[\"Y\"],\n",
    "            \"DMP3\": row[\"Z\"],\n",
    "        })\n",
    "        participant_id += 1\n",
    "\n",
    "# Save base assignments\n",
    "with open(ASSIGN_CSV, \"w\", newline=\"\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"Participant number\", \"DMP1\", \"DMP2\", \"DMP3\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(assignments)\n",
    "print(f\"âœ… Saved {ASSIGN_CSV} ({len(assignments)} records)\")\n",
    "\n",
    "# ---------------- Master link table maintenance ----------------\n",
    "def load_links(path):\n",
    "    \"\"\"Return dict: { 'X_1.md': 'https://...' } if file exists, else {}.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        return {}\n",
    "    links = {}\n",
    "    with open(path, newline=\"\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            name = row.get(\"dmp_md\", \"\").strip()\n",
    "            link = row.get(\"pdf_link\", \"\").strip()\n",
    "            if name:\n",
    "                links[name] = link\n",
    "    return links\n",
    "\n",
    "def save_links(mapping, path):\n",
    "    \"\"\"Write mapping back (stable sorted by filename).\"\"\"\n",
    "    with open(path, \"w\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"dmp_md\", \"pdf_link\"])\n",
    "        writer.writeheader()\n",
    "        for name in sorted(mapping.keys()):\n",
    "            writer.writerow({\"dmp_md\": name, \"pdf_link\": mapping[name]})\n",
    "\n",
    "def update_link_table(dmp_names, path=LINKS_CSV):\n",
    "    \"\"\"\n",
    "    Ensure all DMP names exist in the master CSV.\n",
    "    Preserve existing links; add new rows with empty link.\n",
    "    \"\"\"\n",
    "    links = load_links(path)\n",
    "    added = 0\n",
    "    for name in dmp_names:\n",
    "        if name not in links:\n",
    "            links[name] = \"\"   # you fill this manually later\n",
    "            added += 1\n",
    "    if added or not os.path.exists(path):\n",
    "        save_links(links, path)\n",
    "    return links, added\n",
    "\n",
    "# Gather all unique DMP filenames used in assignments\n",
    "all_dmps = set()\n",
    "for row in assignments:\n",
    "    all_dmps.add(row[\"DMP1\"])\n",
    "    all_dmps.add(row[\"DMP2\"])\n",
    "    all_dmps.add(row[\"DMP3\"])\n",
    "\n",
    "# Update master link CSV (preserves your manual edits)\n",
    "links_map, num_added = update_link_table(all_dmps, LINKS_CSV)\n",
    "print(f\"âœ… Updated {LINKS_CSV}: {len(links_map)} total rows ({num_added} new).\")\n",
    "print(\"   ðŸ‘‰ Open this CSV and paste your Google Drive PDF links in the 'pdf_link' column.\")\n",
    "\n",
    "# ---------------- Join links into a convenience CSV ----------------\n",
    "with open(ASSIGN_WITH_LINKS_CSV, \"w\", newline=\"\") as f:\n",
    "    fieldnames = [\"Participant number\",\n",
    "                  \"DMP1\", \"DMP1_link\",\n",
    "                  \"DMP2\", \"DMP2_link\",\n",
    "                  \"DMP3\", \"DMP3_link\"]\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for row in assignments:\n",
    "        writer.writerow({\n",
    "            \"Participant number\": row[\"Participant number\"],\n",
    "            \"DMP1\": row[\"DMP1\"],\n",
    "            \"DMP1_link\": links_map.get(row[\"DMP1\"], \"\"),\n",
    "            \"DMP2\": row[\"DMP2\"],\n",
    "            \"DMP2_link\": links_map.get(row[\"DMP2\"], \"\"),\n",
    "            \"DMP3\": row[\"DMP3\"],\n",
    "            \"DMP3_link\": links_map.get(row[\"DMP3\"], \"\"),\n",
    "        })\n",
    "print(f\"âœ… Saved {ASSIGN_WITH_LINKS_CSV} (joined with links)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db8a7f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Row 1: OK  -> X_2.md, Y_10.md, Z_4.md\n",
      "âœ… Row 2: OK  -> X_10.md, Y_4.md, Z_7.md\n",
      "âœ… Row 3: OK  -> X_4.md, Y_7.md, Z_8.md\n",
      "âœ… Row 4: OK  -> X_7.md, Y_8.md, Z_6.md\n",
      "âœ… Row 5: OK  -> X_8.md, Y_6.md, Z_5.md\n",
      "âœ… Row 6: OK  -> X_6.md, Y_5.md, Z_9.md\n",
      "âœ… Row 7: OK  -> X_5.md, Y_9.md, Z_1.md\n",
      "âœ… Row 8: OK  -> X_9.md, Y_1.md, Z_3.md\n",
      "âœ… Row 9: OK  -> X_1.md, Y_3.md, Z_2.md\n",
      "âœ… Row 10: OK  -> X_3.md, Y_2.md, Z_10.md\n",
      "âœ… Row 11: OK  -> X_4.md, Y_1.md, Z_3.md\n",
      "âœ… Row 12: OK  -> X_1.md, Y_3.md, Z_5.md\n",
      "âœ… Row 13: OK  -> X_3.md, Y_5.md, Z_8.md\n",
      "âœ… Row 14: OK  -> X_5.md, Y_8.md, Z_6.md\n",
      "âœ… Row 15: OK  -> X_8.md, Y_6.md, Z_9.md\n",
      "âœ… Row 16: OK  -> X_6.md, Y_9.md, Z_10.md\n",
      "âœ… Row 17: OK  -> X_9.md, Y_10.md, Z_7.md\n",
      "âœ… Row 18: OK  -> X_10.md, Y_7.md, Z_2.md\n",
      "âœ… Row 19: OK  -> X_7.md, Y_2.md, Z_4.md\n",
      "âœ… Row 20: OK  -> X_2.md, Y_4.md, Z_1.md\n",
      "âœ… Row 21: OK  -> X_1.md, Y_2.md, Z_4.md\n",
      "âœ… Row 22: OK  -> X_2.md, Y_4.md, Z_6.md\n",
      "âœ… Row 23: OK  -> X_4.md, Y_6.md, Z_3.md\n",
      "âœ… Row 24: OK  -> X_6.md, Y_3.md, Z_5.md\n",
      "âœ… Row 25: OK  -> X_3.md, Y_5.md, Z_9.md\n",
      "âœ… Row 26: OK  -> X_5.md, Y_9.md, Z_7.md\n",
      "âœ… Row 27: OK  -> X_9.md, Y_7.md, Z_10.md\n",
      "âœ… Row 28: OK  -> X_7.md, Y_10.md, Z_8.md\n",
      "âœ… Row 29: OK  -> X_10.md, Y_8.md, Z_1.md\n",
      "âœ… Row 30: OK  -> X_8.md, Y_1.md, Z_2.md\n",
      "\n",
      "=== Batch coverage check ===\n",
      "âœ… Batch 1: perfect coverage for all categories.\n",
      "âœ… Batch 2: perfect coverage for all categories.\n",
      "âœ… Batch 3: perfect coverage for all categories.\n",
      "\n",
      "â€” Overall distribution per category â€”\n",
      "X:  {1: 3, 2: 3, 3: 3, 4: 3, 5: 3, 6: 3, 7: 3, 8: 3, 9: 3, 10: 3}\n",
      "Y:  {1: 3, 2: 3, 3: 3, 4: 3, 5: 3, 6: 3, 7: 3, 8: 3, 9: 3, 10: 3}\n",
      "Z:  {1: 3, 2: 3, 3: 3, 4: 3, 5: 3, 6: 3, 7: 3, 8: 3, 9: 3, 10: 3}\n",
      "\n",
      "=== Summary ===\n",
      "âœ… All rows valid AND all batches have perfect per-category coverage.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# ---- Config (keep in sync with your generator) ----\n",
    "CATEGORIES = [\"X\", \"Y\", \"Z\"]\n",
    "NUM_DMPS_PER_CATEGORY = 10\n",
    "BATCH_SIZE = NUM_DMPS_PER_CATEGORY           # expecting batches of 10\n",
    "CSV_PATH = \"participant_dmp_assignments_with_links.csv\" # change if needed\n",
    "\n",
    "# ---- Helpers ----\n",
    "file_pattern = re.compile(r\"^([A-Za-z]+)_(\\d+)\\.md$\")  # e.g., X_7.md\n",
    "\n",
    "def parse_cell(cell):\n",
    "    \"\"\"Return (category, number) if valid, else raise ValueError.\"\"\"\n",
    "    m = file_pattern.match(cell.strip())\n",
    "    if not m:\n",
    "        raise ValueError(f\"Bad filename format: {cell!r}\")\n",
    "    cat, num = m.group(1), int(m.group(2))\n",
    "    if cat not in CATEGORIES:\n",
    "        raise ValueError(f\"Unknown category {cat!r} in {cell!r}\")\n",
    "    if not (1 <= num <= NUM_DMPS_PER_CATEGORY):\n",
    "        raise ValueError(f\"Number out of range in {cell!r} (must be 1..{NUM_DMPS_PER_CATEGORY})\")\n",
    "    return cat, num\n",
    "\n",
    "def validate_row(row):\n",
    "    \"\"\"\n",
    "    Validate a single assignment row.\n",
    "    - Has DMP1..DMP3\n",
    "    - Each DMP matches pattern\n",
    "    - Exactly one from each category (no duplicate categories)\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    dmps = []\n",
    "    for key in (\"DMP1\",\"DMP2\",\"DMP3\"):\n",
    "        if key not in row or not row[key]:\n",
    "            errors.append(f\"Missing {key}\")\n",
    "            continue\n",
    "        try:\n",
    "            dmps.append(parse_cell(row[key]))\n",
    "        except ValueError as e:\n",
    "            errors.append(str(e))\n",
    "\n",
    "    # If parsing failed for any, stop further checks\n",
    "    if errors:\n",
    "        return False, errors, dmps\n",
    "\n",
    "    cats = [c for c,_ in dmps]\n",
    "    if len(set(cats)) != 3:\n",
    "        errors.append(f\"Duplicate/missing categories in row: {cats}. Expected one each of {CATEGORIES}.\")\n",
    "    if set(cats) != set(CATEGORIES):\n",
    "        missing = set(CATEGORIES) - set(cats)\n",
    "        extra = set(cats) - set(CATEGORIES)\n",
    "        if missing:\n",
    "            errors.append(f\"Missing categories: {sorted(missing)}\")\n",
    "        if extra:\n",
    "            errors.append(f\"Unexpected categories: {sorted(extra)}\")\n",
    "\n",
    "    return (len(errors) == 0), errors, dmps\n",
    "\n",
    "def validate_batches(rows):\n",
    "    \"\"\"\n",
    "    Check that in each 10-participant batch, each category covers numbers 1..10 exactly once.\n",
    "    \"\"\"\n",
    "    ok = True\n",
    "    for start in range(0, len(rows), BATCH_SIZE):\n",
    "        batch = rows[start:start+BATCH_SIZE]\n",
    "        # Track seen numbers per category within the batch\n",
    "        seen = {cat: set() for cat in CATEGORIES}\n",
    "        for r in batch:\n",
    "            for key in (\"DMP1\",\"DMP2\",\"DMP3\"):\n",
    "                try:\n",
    "                    cat, num = parse_cell(r[key])\n",
    "                    if cat in seen:\n",
    "                        seen[cat].add(num)\n",
    "                except Exception:\n",
    "                    # Row-level errors will be reported elsewhere\n",
    "                    pass\n",
    "\n",
    "        expected = set(range(1, NUM_DMPS_PER_CATEGORY + 1))\n",
    "        batch_idx = start // BATCH_SIZE + 1\n",
    "        for cat in CATEGORIES:\n",
    "            if set(seen[cat]) != expected:\n",
    "                ok = False\n",
    "                missing = sorted(expected - seen[cat])\n",
    "                extra   = sorted(seen[cat] - expected)\n",
    "                msg = []\n",
    "                if missing: msg.append(f\"missing {missing}\")\n",
    "                if extra:   msg.append(f\"extra {extra}\")\n",
    "                print(f\"âŒ Batch {batch_idx} category {cat}: does not cover 1..{NUM_DMPS_PER_CATEGORY} exactly once ({'; '.join(msg)})\")\n",
    "        if all(set(seen[c]) == expected for c in CATEGORIES):\n",
    "            print(f\"âœ… Batch {batch_idx}: perfect coverage for all categories.\")\n",
    "    return ok\n",
    "\n",
    "def overall_distribution(rows):\n",
    "    \"\"\"\n",
    "    Optional: show how many times each DMP number appears per category across ALL participants.\n",
    "    (With 30 participants in 3 equal batches, each number should appear exactly 3 times per category.)\n",
    "    \"\"\"\n",
    "    counts = {cat: defaultdict(int) for cat in CATEGORIES}\n",
    "    for r in rows:\n",
    "        for key in (\"DMP1\",\"DMP2\",\"DMP3\"):\n",
    "            try:\n",
    "                cat, num = parse_cell(r[key])\n",
    "                counts[cat][num] += 1\n",
    "            except Exception:\n",
    "                pass\n",
    "    print(\"\\nâ€” Overall distribution per category â€”\")\n",
    "    for cat in CATEGORIES:\n",
    "        print(f\"{cat}: \", {n: counts[cat][n] for n in range(1, NUM_DMPS_PER_CATEGORY+1)})\n",
    "\n",
    "# ---- Load CSV ----\n",
    "with open(CSV_PATH, newline=\"\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    rows = list(reader)\n",
    "\n",
    "# ---- Row-by-row validation ----\n",
    "all_rows_ok = True\n",
    "for r in rows:\n",
    "    pid = r.get(\"Participant number\", \"?\")\n",
    "    ok, errors, dmps = validate_row(r)\n",
    "    if ok:\n",
    "        print(f\"âœ… Row {pid}: OK  -> {r['DMP1']}, {r['DMP2']}, {r['DMP3']}\")\n",
    "    else:\n",
    "        all_rows_ok = False\n",
    "        print(f\"âŒ Row {pid}:\")\n",
    "        for e in errors:\n",
    "            print(\"   -\", e)\n",
    "\n",
    "# ---- Batch-level coverage validation ----\n",
    "print(\"\\n=== Batch coverage check ===\")\n",
    "batches_ok = validate_batches(rows)\n",
    "\n",
    "# ---- Optional overall distribution summary ----\n",
    "overall_distribution(rows)\n",
    "\n",
    "# ---- Final verdict ----\n",
    "print(\"\\n=== Summary ===\")\n",
    "if all_rows_ok and batches_ok:\n",
    "    print(\"âœ… All rows valid AND all batches have perfect per-category coverage.\")\n",
    "elif not all_rows_ok and not batches_ok:\n",
    "    print(\"âŒ Some rows are invalid AND batch coverage is not satisfied. See messages above.\")\n",
    "elif not all_rows_ok:\n",
    "    print(\"âŒ Some rows are invalid. Batch coverage may still be OK; see messages above.\")\n",
    "else:\n",
    "    print(\"âŒ All rows valid, but batch coverage FAILED. See messages above.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91366fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved participant_dmp_assignments.csv (30 records)\n",
      "âœ… Updated dmp_links.csv: 30 total rows (30 new).\n",
      "   ðŸ‘‰ Open this CSV and paste your Google Drive PDF links in the 'pdf_link' column.\n",
      "âœ… Saved participant_dmp_assignments_with_links.csv (joined with links)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import csv\n",
    "from math import ceil\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "CATEGORIES = [\"X\", \"Y\", \"Z\"]\n",
    "NUM_DMPS_PER_CATEGORY = 10\n",
    "NUM_PARTICIPANTS = 30\n",
    "BATCH_SIZE = NUM_DMPS_PER_CATEGORY  # 10 participants per batch\n",
    "LINKS_CSV = \"dmp_links.csv\"         # master link table you edit by hand\n",
    "ASSIGN_CSV = \"participant_dmp_assignments.csv\"\n",
    "ASSIGN_WITH_LINKS_CSV = \"participant_dmp_assignments_with_links.csv\"\n",
    "\n",
    "random.seed()  # set e.g. random.seed(42) for reproducibility\n",
    "\n",
    "# ---------------- Assignment generation (rotation ensures distinct numbers per row) ----------------\n",
    "assignments = []\n",
    "participant_id = 1\n",
    "num_batches = ceil(NUM_PARTICIPANTS / BATCH_SIZE)\n",
    "\n",
    "for b in range(num_batches):\n",
    "    remaining = NUM_PARTICIPANTS - participant_id + 1\n",
    "    this_batch_size = min(BATCH_SIZE, remaining)\n",
    "\n",
    "    base = list(range(1, NUM_DMPS_PER_CATEGORY + 1))\n",
    "    random.shuffle(base)\n",
    "\n",
    "    shifts = {\"X\": 0, \"Y\": 1, \"Z\": 2}  # fixed rotation\n",
    "\n",
    "    for i in range(this_batch_size):\n",
    "        row = {}\n",
    "        for cat in CATEGORIES:\n",
    "            num = base[(i + shifts[cat]) % NUM_DMPS_PER_CATEGORY]\n",
    "            row[cat] = f\"{cat}_{num}.md\"\n",
    "\n",
    "        assignments.append({\n",
    "            \"Participant number\": participant_id,\n",
    "            \"DMP1\": row[\"X\"],\n",
    "            \"DMP2\": row[\"Y\"],\n",
    "            \"DMP3\": row[\"Z\"],\n",
    "        })\n",
    "        participant_id += 1\n",
    "\n",
    "# Save base assignments\n",
    "with open(ASSIGN_CSV, \"w\", newline=\"\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"Participant number\", \"DMP1\", \"DMP2\", \"DMP3\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(assignments)\n",
    "print(f\"âœ… Saved {ASSIGN_CSV} ({len(assignments)} records)\")\n",
    "\n",
    "# ---------------- Master link table maintenance ----------------\n",
    "def load_links(path):\n",
    "    \"\"\"Return dict: { 'X_1.md': 'https://...' } if file exists, else {}.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        return {}\n",
    "    links = {}\n",
    "    with open(path, newline=\"\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            name = row.get(\"dmp_md\", \"\").strip()\n",
    "            link = row.get(\"pdf_link\", \"\").strip()\n",
    "            if name:\n",
    "                links[name] = link\n",
    "    return links\n",
    "\n",
    "def save_links(mapping, path):\n",
    "    \"\"\"Write mapping back (stable sorted by filename).\"\"\"\n",
    "    with open(path, \"w\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"dmp_md\", \"pdf_link\"])\n",
    "        writer.writeheader()\n",
    "        for name in sorted(mapping.keys()):\n",
    "            writer.writerow({\"dmp_md\": name, \"pdf_link\": mapping[name]})\n",
    "\n",
    "def update_link_table(dmp_names, path=LINKS_CSV):\n",
    "    \"\"\"\n",
    "    Ensure all DMP names exist in the master CSV.\n",
    "    Preserve existing links; add new rows with empty link.\n",
    "    \"\"\"\n",
    "    links = load_links(path)\n",
    "    added = 0\n",
    "    for name in dmp_names:\n",
    "        if name not in links:\n",
    "            links[name] = \"\"   # you fill this manually later\n",
    "            added += 1\n",
    "    if added or not os.path.exists(path):\n",
    "        save_links(links, path)\n",
    "    return links, added\n",
    "\n",
    "# Gather all unique DMP filenames used in assignments\n",
    "all_dmps = set()\n",
    "for row in assignments:\n",
    "    all_dmps.add(row[\"DMP1\"])\n",
    "    all_dmps.add(row[\"DMP2\"])\n",
    "    all_dmps.add(row[\"DMP3\"])\n",
    "\n",
    "# Update master link CSV (preserves your manual edits)\n",
    "links_map, num_added = update_link_table(all_dmps, LINKS_CSV)\n",
    "print(f\"âœ… Updated {LINKS_CSV}: {len(links_map)} total rows ({num_added} new).\")\n",
    "print(\"   ðŸ‘‰ Open this CSV and paste your Google Drive PDF links in the 'pdf_link' column.\")\n",
    "\n",
    "# ---------------- Join links into a convenience CSV ----------------\n",
    "with open(ASSIGN_WITH_LINKS_CSV, \"w\", newline=\"\") as f:\n",
    "    fieldnames = [\"Participant number\",\n",
    "                  \"DMP1\", \"DMP1_link\",\n",
    "                  \"DMP2\", \"DMP2_link\",\n",
    "                  \"DMP3\", \"DMP3_link\"]\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for row in assignments:\n",
    "        writer.writerow({\n",
    "            \"Participant number\": row[\"Participant number\"],\n",
    "            \"DMP1\": row[\"DMP1\"],\n",
    "            \"DMP1_link\": links_map.get(row[\"DMP1\"], \"\"),\n",
    "            \"DMP2\": row[\"DMP2\"],\n",
    "            \"DMP2_link\": links_map.get(row[\"DMP2\"], \"\"),\n",
    "            \"DMP3\": row[\"DMP3\"],\n",
    "            \"DMP3_link\": links_map.get(row[\"DMP3\"], \"\"),\n",
    "        })\n",
    "print(f\"âœ… Saved {ASSIGN_WITH_LINKS_CSV} (joined with links)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f9b7313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as participant_dmp_anonymized.csv and participant_dmp_mapping.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Original table\n",
    "data = {\n",
    "    \"Participant number\": [\n",
    "        \"2170\", \"4830\", \"1379\", \"2205\", \"3091\", \"4126\", \"5340\", \"6078\", \"7124\",\n",
    "        \"8053\", \"8296\", \"9012\", \"9185\", \"9427\", \"9963\"\n",
    "    ],\n",
    "    \"DMP1\": [\n",
    "        \"Human_4\", \"Llama_1\", \"Human_7\", \"Gpt_3\", \"Gpt_1\", \"Human_6\", \"Human_8\",\n",
    "        \"Human_3\", \"Gpt_2\", \"Gpt_7\", \"Llama_7\", \"Llama_8\", \"Gpt_9\", \"Llama_5\", \"Llama_4\"\n",
    "    ],\n",
    "    \"DMP2\": [\n",
    "        \"Llama_10\", \"Human_2\", \"Gpt_6\", \"Human_9\", \"Human_1\", \"Gpt_5\", \"Llama_4\",\n",
    "        \"Llama_7\", \"Llama_2\", \"Llama_8\", \"Gpt_7\", \"Gpt_1\", \"Human_9\", \"Gpt_3\", \"Gpt_4\"\n",
    "    ],\n",
    "    \"DMP3\": [\n",
    "        \"Gpt_10\", \"Gpt_9\", \"Llama_6\", \"Llama_5\", \"Llama_3\", \"Llama_9\", \"Gpt_4\",\n",
    "        \"Gpt_8\", \"Human_5\", \"Human_10\", \"Human_3\", \"Human_5\", \"Llama_6\", \"Human_2\", \"Human_1\"\n",
    "    ]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create user-facing anonymized table and mapping table\n",
    "user_df = df.copy()\n",
    "mapping_list = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    pid = row['Participant number']\n",
    "    for i, dmp_col in enumerate(['DMP1', 'DMP2', 'DMP3'], start=1):\n",
    "        anon_code = f\"{pid}_{i}\"\n",
    "        user_df.at[idx, dmp_col] = anon_code\n",
    "        mapping_list.append({'encrypt_Code': anon_code, 'Participant number': pid, 'DMP': dmp_col, 'Actual_Model': row[dmp_col]})\n",
    "\n",
    "mapping_df = pd.DataFrame(mapping_list)\n",
    "\n",
    "# Save as CSV files\n",
    "user_df.to_csv('participant_dmp_encrypt.csv', index=False)\n",
    "mapping_df.to_csv('participant_dmp_mapping.csv', index=False)\n",
    "\n",
    "print(\"Saved as participant_dmp_anonymized.csv and participant_dmp_mapping.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8953283f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied: Clean DMP\\Human_4.docx -> Clean DMP-encrypt\\2170_1.docx\n",
      "Copied: Clean DMP\\Human_4.md -> Clean DMP-encrypt\\2170_1.md\n",
      "Copied: Clean DMP\\Llama_10.docx -> Clean DMP-encrypt\\2170_2.docx\n",
      "Copied: Clean DMP\\Llama_10.md -> Clean DMP-encrypt\\2170_2.md\n",
      "Copied: Clean DMP\\Gpt_10.docx -> Clean DMP-encrypt\\2170_3.docx\n",
      "Copied: Clean DMP\\Gpt_10.md -> Clean DMP-encrypt\\2170_3.md\n",
      "Copied: Clean DMP\\Llama_1.docx -> Clean DMP-encrypt\\4830_1.docx\n",
      "Copied: Clean DMP\\Llama_1.md -> Clean DMP-encrypt\\4830_1.md\n",
      "Copied: Clean DMP\\Human_2.docx -> Clean DMP-encrypt\\4830_2.docx\n",
      "Copied: Clean DMP\\Human_2.md -> Clean DMP-encrypt\\4830_2.md\n",
      "Copied: Clean DMP\\Gpt_9.docx -> Clean DMP-encrypt\\4830_3.docx\n",
      "Copied: Clean DMP\\Gpt_9.md -> Clean DMP-encrypt\\4830_3.md\n",
      "Copied: Clean DMP\\Human_7.docx -> Clean DMP-encrypt\\1379_1.docx\n",
      "Copied: Clean DMP\\Human_7.md -> Clean DMP-encrypt\\1379_1.md\n",
      "Copied: Clean DMP\\Gpt_6.docx -> Clean DMP-encrypt\\1379_2.docx\n",
      "Copied: Clean DMP\\Gpt_6.md -> Clean DMP-encrypt\\1379_2.md\n",
      "Copied: Clean DMP\\Llama_6.docx -> Clean DMP-encrypt\\1379_3.docx\n",
      "Copied: Clean DMP\\Llama_6.md -> Clean DMP-encrypt\\1379_3.md\n",
      "Copied: Clean DMP\\Gpt_3.docx -> Clean DMP-encrypt\\2205_1.docx\n",
      "Copied: Clean DMP\\Gpt_3.md -> Clean DMP-encrypt\\2205_1.md\n",
      "Copied: Clean DMP\\Human_9.docx -> Clean DMP-encrypt\\2205_2.docx\n",
      "Copied: Clean DMP\\Human_9.md -> Clean DMP-encrypt\\2205_2.md\n",
      "Copied: Clean DMP\\Llama_5.docx -> Clean DMP-encrypt\\2205_3.docx\n",
      "Copied: Clean DMP\\Llama_5.md -> Clean DMP-encrypt\\2205_3.md\n",
      "Copied: Clean DMP\\Gpt_1.docx -> Clean DMP-encrypt\\3091_1.docx\n",
      "Copied: Clean DMP\\Gpt_1.md -> Clean DMP-encrypt\\3091_1.md\n",
      "Copied: Clean DMP\\Human_1.docx -> Clean DMP-encrypt\\3091_2.docx\n",
      "Copied: Clean DMP\\Human_1.md -> Clean DMP-encrypt\\3091_2.md\n",
      "Copied: Clean DMP\\Llama_3.docx -> Clean DMP-encrypt\\3091_3.docx\n",
      "Copied: Clean DMP\\Llama_3.md -> Clean DMP-encrypt\\3091_3.md\n",
      "Copied: Clean DMP\\Human_6.docx -> Clean DMP-encrypt\\4126_1.docx\n",
      "Copied: Clean DMP\\Human_6.md -> Clean DMP-encrypt\\4126_1.md\n",
      "Copied: Clean DMP\\Gpt_5.docx -> Clean DMP-encrypt\\4126_2.docx\n",
      "Copied: Clean DMP\\Gpt_5.md -> Clean DMP-encrypt\\4126_2.md\n",
      "Copied: Clean DMP\\Llama_9.docx -> Clean DMP-encrypt\\4126_3.docx\n",
      "Copied: Clean DMP\\Llama_9.md -> Clean DMP-encrypt\\4126_3.md\n",
      "Copied: Clean DMP\\Human_8.docx -> Clean DMP-encrypt\\5340_1.docx\n",
      "Copied: Clean DMP\\Human_8.md -> Clean DMP-encrypt\\5340_1.md\n",
      "Copied: Clean DMP\\Llama_4.docx -> Clean DMP-encrypt\\5340_2.docx\n",
      "Copied: Clean DMP\\Llama_4.md -> Clean DMP-encrypt\\5340_2.md\n",
      "Copied: Clean DMP\\Gpt_4.docx -> Clean DMP-encrypt\\5340_3.docx\n",
      "Copied: Clean DMP\\Gpt_4.md -> Clean DMP-encrypt\\5340_3.md\n",
      "Copied: Clean DMP\\Human_3.docx -> Clean DMP-encrypt\\6078_1.docx\n",
      "Copied: Clean DMP\\Human_3.md -> Clean DMP-encrypt\\6078_1.md\n",
      "Copied: Clean DMP\\Llama_7.docx -> Clean DMP-encrypt\\6078_2.docx\n",
      "Copied: Clean DMP\\Llama_7.md -> Clean DMP-encrypt\\6078_2.md\n",
      "Copied: Clean DMP\\Gpt_8.docx -> Clean DMP-encrypt\\6078_3.docx\n",
      "Copied: Clean DMP\\Gpt_8.md -> Clean DMP-encrypt\\6078_3.md\n",
      "Copied: Clean DMP\\Gpt_2.docx -> Clean DMP-encrypt\\7124_1.docx\n",
      "Copied: Clean DMP\\Gpt_2.md -> Clean DMP-encrypt\\7124_1.md\n",
      "Copied: Clean DMP\\Llama_2.docx -> Clean DMP-encrypt\\7124_2.docx\n",
      "Copied: Clean DMP\\Llama_2.md -> Clean DMP-encrypt\\7124_2.md\n",
      "Copied: Clean DMP\\Human_5.docx -> Clean DMP-encrypt\\7124_3.docx\n",
      "Copied: Clean DMP\\Human_5.md -> Clean DMP-encrypt\\7124_3.md\n",
      "Copied: Clean DMP\\Gpt_7.docx -> Clean DMP-encrypt\\8053_1.docx\n",
      "Copied: Clean DMP\\Gpt_7.md -> Clean DMP-encrypt\\8053_1.md\n",
      "Copied: Clean DMP\\Llama_8.docx -> Clean DMP-encrypt\\8053_2.docx\n",
      "Copied: Clean DMP\\Llama_8.md -> Clean DMP-encrypt\\8053_2.md\n",
      "Copied: Clean DMP\\Human_10.docx -> Clean DMP-encrypt\\8053_3.docx\n",
      "Copied: Clean DMP\\Human_10.md -> Clean DMP-encrypt\\8053_3.md\n",
      "Copied: Clean DMP\\Llama_7.docx -> Clean DMP-encrypt\\8296_1.docx\n",
      "Copied: Clean DMP\\Llama_7.md -> Clean DMP-encrypt\\8296_1.md\n",
      "Copied: Clean DMP\\Gpt_7.docx -> Clean DMP-encrypt\\8296_2.docx\n",
      "Copied: Clean DMP\\Gpt_7.md -> Clean DMP-encrypt\\8296_2.md\n",
      "Copied: Clean DMP\\Human_3.docx -> Clean DMP-encrypt\\8296_3.docx\n",
      "Copied: Clean DMP\\Human_3.md -> Clean DMP-encrypt\\8296_3.md\n",
      "Copied: Clean DMP\\Llama_8.docx -> Clean DMP-encrypt\\9012_1.docx\n",
      "Copied: Clean DMP\\Llama_8.md -> Clean DMP-encrypt\\9012_1.md\n",
      "Copied: Clean DMP\\Gpt_1.docx -> Clean DMP-encrypt\\9012_2.docx\n",
      "Copied: Clean DMP\\Gpt_1.md -> Clean DMP-encrypt\\9012_2.md\n",
      "Copied: Clean DMP\\Human_5.docx -> Clean DMP-encrypt\\9012_3.docx\n",
      "Copied: Clean DMP\\Human_5.md -> Clean DMP-encrypt\\9012_3.md\n",
      "Copied: Clean DMP\\Gpt_9.docx -> Clean DMP-encrypt\\9185_1.docx\n",
      "Copied: Clean DMP\\Gpt_9.md -> Clean DMP-encrypt\\9185_1.md\n",
      "Copied: Clean DMP\\Human_9.docx -> Clean DMP-encrypt\\9185_2.docx\n",
      "Copied: Clean DMP\\Human_9.md -> Clean DMP-encrypt\\9185_2.md\n",
      "Copied: Clean DMP\\Llama_6.docx -> Clean DMP-encrypt\\9185_3.docx\n",
      "Copied: Clean DMP\\Llama_6.md -> Clean DMP-encrypt\\9185_3.md\n",
      "Copied: Clean DMP\\Llama_5.docx -> Clean DMP-encrypt\\9427_1.docx\n",
      "Copied: Clean DMP\\Llama_5.md -> Clean DMP-encrypt\\9427_1.md\n",
      "Copied: Clean DMP\\Gpt_3.docx -> Clean DMP-encrypt\\9427_2.docx\n",
      "Copied: Clean DMP\\Gpt_3.md -> Clean DMP-encrypt\\9427_2.md\n",
      "Copied: Clean DMP\\Human_2.docx -> Clean DMP-encrypt\\9427_3.docx\n",
      "Copied: Clean DMP\\Human_2.md -> Clean DMP-encrypt\\9427_3.md\n",
      "Copied: Clean DMP\\Llama_4.docx -> Clean DMP-encrypt\\9963_1.docx\n",
      "Copied: Clean DMP\\Llama_4.md -> Clean DMP-encrypt\\9963_1.md\n",
      "Copied: Clean DMP\\Gpt_4.docx -> Clean DMP-encrypt\\9963_2.docx\n",
      "Copied: Clean DMP\\Gpt_4.md -> Clean DMP-encrypt\\9963_2.md\n",
      "Copied: Clean DMP\\Human_1.docx -> Clean DMP-encrypt\\9963_3.docx\n",
      "Copied: Clean DMP\\Human_1.md -> Clean DMP-encrypt\\9963_3.md\n",
      "All files processed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "# Paths\n",
    "mapping_csv = 'participant_dmp_mapping.csv'\n",
    "input_folder = 'Clean DMP'       # Update to your folder with original files\n",
    "output_folder = 'Clean DMP-encrypt'     # Update to your desired output folder\n",
    "\n",
    "# Ensure output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Load mapping\n",
    "mapping_df = pd.read_csv(mapping_csv)\n",
    "\n",
    "for _, row in mapping_df.iterrows():\n",
    "    actual_model = row['Actual_Model']\n",
    "    encrypt_code = row['encrypt_Code']\n",
    "    \n",
    "    for ext in ['.docx', '.md']:\n",
    "        src_file = os.path.join(input_folder, actual_model + ext)\n",
    "        dst_file = os.path.join(output_folder, encrypt_code + ext)\n",
    "        \n",
    "        if os.path.exists(src_file):\n",
    "            shutil.copy2(src_file, dst_file)\n",
    "            print(f\"Copied: {src_file} -> {dst_file}\")\n",
    "        else:\n",
    "            print(f\"WARNING: File does not exist: {src_file}\")\n",
    "\n",
    "print(\"All files processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2ff83e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'docx2pdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Optional: Install docx2pdf if not already installed\u001b[39;00m\n\u001b[32m      6\u001b[39m get_ipython().system(\u001b[33m'\u001b[39m\u001b[33m pip install docx2pdf\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdocx2pdf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m convert\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Paths\u001b[39;00m\n\u001b[32m     11\u001b[39m mapping_csv = \u001b[33m'\u001b[39m\u001b[33mparticipant_dmp_mapping.csv\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'docx2pdf'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "# Optional: Install docx2pdf if not already installed\n",
    "! pip install docx2pdf\n",
    "\n",
    "from docx2pdf import convert\n",
    "\n",
    "# Paths\n",
    "mapping_csv = 'participant_dmp_mapping.csv'\n",
    "input_folder = 'Clean DMP'\n",
    "output_folder = 'Clean DMP-encrypt'\n",
    "\n",
    "# Ensure output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Load mapping\n",
    "mapping_df = pd.read_csv(mapping_csv)\n",
    "\n",
    "# Copy and rename files\n",
    "for _, row in mapping_df.iterrows():\n",
    "    actual_model = row['Actual_Model']\n",
    "    encrypt_code = row['encrypt_Code']\n",
    "    \n",
    "    for ext in ['.docx', '.md']:\n",
    "        src_file = os.path.join(input_folder, actual_model + ext)\n",
    "        dst_file = os.path.join(output_folder, encrypt_code + ext)\n",
    "        \n",
    "        if os.path.exists(src_file):\n",
    "            shutil.copy2(src_file, dst_file)\n",
    "            print(f\"Copied: {src_file} -> {dst_file}\")\n",
    "        else:\n",
    "            print(f\"WARNING: File does not exist: {src_file}\")\n",
    "\n",
    "print(\"All files processed.\")\n",
    "\n",
    "# --- Convert .docx files in output folder to .pdf ---\n",
    "for fname in os.listdir(output_folder):\n",
    "    if fname.endswith('.docx'):\n",
    "        docx_path = os.path.join(output_folder, fname)\n",
    "        pdf_path = os.path.splitext(docx_path)[0] + '.pdf'\n",
    "        try:\n",
    "            convert(docx_path, pdf_path)\n",
    "            print(f\"Converted: {docx_path} -> {pdf_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR converting {docx_path}: {e}\")\n",
    "\n",
    "print(\"All .docx files converted to PDF.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
